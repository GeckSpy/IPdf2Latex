{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f637c2",
   "metadata": {},
   "source": [
    "# *Introduction to few key NLP metrics*\n",
    "\n",
    "In any scenario, where there is a need to judge how good a machine translation of a given source language is, or judge the performance of a classifier, we need to introduce metrics. Note that, in case of machine translation source and target language can be anything--a PDF, latex code corresponding to a PDF, any human language, etc. Hence these metrics are language-independent. We attempt to introduce few of these metrics here:\n",
    "\n",
    "+ Levenshtein distance (Edit distance)\n",
    "+ BLEU (BiLingual Evaluation Understudy)\n",
    "+ ROUGE (Recall Oriented Understudy for Gisting Evaluation)\n",
    "+ Precision\n",
    "+ Accuracy\n",
    "+ Recall\n",
    "+ F1 Score\n",
    "\n",
    "## Machine Translations related\n",
    "\n",
    "### Levenshtein distance\n",
    " Levenshtein or Edit distance is a fundamental string metric. Given two strings $A$ and $B$, it computes minimum number of insertions, deletions or replacements needed to transform $A$ to $B$. It helps to compute how much different a machine translated string is from the source string. Edit distance has a convenient recursive formula:\n",
    " \n",
    " $$lev_{a,b}(i,j) = \\begin{cases} \n",
    "          lev_{a,b}(i+1,j+1) & if \\ a[i]=b[j], a,b\\neq\\phi \\\\\n",
    "          1+min\\{lev_{a,b}(i,j+1),lev_{a,b}(i+1,j),lev_{a,b}(i+1,j+1)\\} & if \\ a[i] \\neq b[j], a,b\\neq \\phi \\\\\n",
    "          |a[i:]| & if \\ b=\\phi \\\\\n",
    "          |b[j:]| & if \\ b\\neq\\phi, a=\\phi\n",
    "       \\end{cases}$$\n",
    "       \n",
    "where <mark>$lev_{a,b}(i,j)$ is the levenshtein distance of the string $a$ starting from $i^{th}$ index w.r.t. string $b$ starting from $j^{th}$ index</mark>. \n",
    "\n",
    "*EXPLANATION*: In order to compute $lev_{a,b}(i,j)$:\n",
    "+ If $a[i] = b[j]$, then it is quite clear that we recursively compute the Edit distance of the string $a[i+1:]$ w.r.t. $b[j+1:]\n",
    " $\n",
    "+ If $a[i] \\neq b[j]$, then either:\n",
    "  + we add $b[j]$ in the current position of $a$, so we recursively compute the Edit distance of the string $a[i:]$ w.r.t. $b[j+1:]$ (as $b[j]$ has been matched but didn't exhaust any character of the original $a$) and add $1$ to result(for the $Add$ operation).\n",
    "  + we replace $a[i]$ with $b[j]$, so we recursively compute the Edit distance of the string $a[i+1:]$ w.r.t. $b[j+1:]$ (since $b[j]$ has been matched, and exhausted one more character of $a$) and add $1$ to result(for the $Replace$ operation).\n",
    "  + we delete $a[i]$ in hope that the rest of the $a$ string matches with rest of the $b$ string, so we recursively compute Edit distance of $a[i+1:]$ w.r.t. $b[j]$(since character of $a$ has been exhausted without matching $b[j]$)\n",
    " \n",
    "The recursive formula, naturally leads to a bottom up dynamic programming approach, where we take a $(|a|+1)$x$(|b|+1)$ matrix $dp$ with rows indexed by $a$ and column by $b$ such that (we output $dp[0][0]=lev_{a,b}(0,0)$ at the end):\n",
    "$$ dp[i][j] =\\begin{cases}\n",
    "lev_{a,b}(i,j) & if \\ 0\\leq i\\leq|a|, 0\\leq j\\leq|b|\\\\\n",
    "lev_{a,\\phi}(i,j) & if \\ j=|b| \\\\\n",
    "leb_{\\phi,b}(i,j) & if \\ i=|a|\\\\\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ce0e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in O(len(str1)*len(str2))\n",
    "def levenshtein( str1, str2):\n",
    "    l1 = len(str1)\n",
    "    l2 = len(str2)\n",
    "    matrix = [[-1]*(l2+1) for i in range(0,l1+1)] # initialisation of the (len(str1)+1)x(len(str2)+1) matrix with -1(s)\n",
    "    \n",
    "    # base cases\n",
    "    for i in range(l2+1):\n",
    "        matrix[l1][i] = l2 - i # initialisation of the bottom row\n",
    "    \n",
    "    for j in range(l1+1):\n",
    "        matrix[j][l2] = l1 - j # initialisation of the rightmost column\n",
    "        \n",
    "    # computation starts!\n",
    "    for i in range(l1-1,-1,-1): # bottom-up fashion--- from second last row to top row, and in each row from second-last cell to starting cell\n",
    "        for j in range(l2-1,-1,-1):\n",
    "            if str1[i]==str2[j]:\n",
    "                matrix[i][j] = matrix[i+1][j+1] # if the characters are equal, move both pointers\n",
    "            else:\n",
    "                matrix[i][j] = 1 + min(matrix[i+1][j],matrix[i+1][j+1], matrix[i][j+1]) # else, add 1 (pertaining to either insert, replace or delete) to the minimum of the levenshstein distances of the remaining pieces of the strings\n",
    "            \n",
    "    \n",
    "    return matrix[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b193c0",
   "metadata": {},
   "source": [
    "Examples:\n",
    "+ levenshtein distance of *\"abc\"* w.r.t *\"adc\"* is $1$ : *\"abc\"*  $\\xrightarrow[]{\\text{replace \"b\" with \"d\"}}$ *\"adc\"*\n",
    "+ levenshtein distance of *\"horse\"* w.r.t. *\"ros\"* is $3$ : *\"horse\"* $\\xrightarrow[]{\\text{replace \"h\" with \"r\"}}$ *\"rorse\"* $\\xrightarrow[]{\\text{delete \"r\"}}$ *\"rose\"* $\\xrightarrow[]{\\text{delete \"e\"}}$ *\"ros\"* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db9f4f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"abc\", \"adc\"))\n",
    "print(levenshtein(\"horse\",\"ros\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc4e9b",
   "metadata": {},
   "source": [
    "### BLEU metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c4b61",
   "metadata": {},
   "source": [
    "<a id='one'></a><mark>Definition 1 (*$n-gram$*)</mark>: Generally an $n-gram$ is a sequence of $n$ consecutive words, letters, syllables or base pairs (*for machine translation(MT) purposes they are generally words*) collected from a text corpus(dataset).\n",
    "\n",
    "Ex: bigrams($2$-grams) of *\"This article is on NLP\"* are *\"This article\"*, *\"article is\"*, *\"on NLP\"*, etc. \n",
    "\n",
    "Given an Machine Translation of a source \"sentence\", and one or more \"references\", we need to able to say how close the translaton is w.r.t the references and clearly distinguish a good candidate from a bad one.\n",
    "\n",
    "<mark>Definition 2(*modified $n-gram$ precision*)</mark> Modified $n-gram$ precision of a candidate sentence w.r.t. one or more references is \n",
    "\n",
    "$$ \\frac{\\sum\\limits_{\\mathcal{C}\\in unique \\  n-grams} \\ count\\_clip(\\mathcal{C})}{number \\ of \\ n-grams}$$, \n",
    "\n",
    "where \n",
    "$count\\_clip(\\epsilon) = min(Count, Max\\_Ref\\_Count)$ , $Count$=number of times the $n-gram$ $\\epsilon$ appears in the candidate sentence, $Max\\_Ref\\_Count$ is the maximum Count of $\\epsilon$ over all reference(sentences).\n",
    "\n",
    "We can calculate the modified $n-gram$ precision using `nltk.translate.bleu_score.modified_precision` method of the NLTK library. (install NLTK using `pip install nltk`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac18e215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# example 1\n",
    "\n",
    "\n",
    "reference  = '\\dfrac{1}{\\sqrt{n} \\Sigma_{i=1}^{n} |i \\rangle'\n",
    "ref        = reference.split()\n",
    "\n",
    "candidate1 = '\\dfrac{1}{\\sqrt{n} \\Sigma\\limits_{i=1}^{n} |i \\rangle'\n",
    "cand       = candidate1.split()\n",
    "print(\"Number of unigrams =\",len(cand))\n",
    "references = [ref]\n",
    "float(nltk.translate.bleu_score.modified_precision(references, cand, n=1)) # modified 1-gram precision = 37/44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18f23b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07692307692307693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 2\n",
    "\n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'.split()\n",
    "reference2 = 'It is the guiding principle which gurantees the military forces always being under the command of the Party'.split()\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'.split()\n",
    "\n",
    "candidate1 = 'It is to insure the troops forever hearing the activity guidebook that party direct'.split()\n",
    "references = [reference1, reference2, reference3]\n",
    "print(float(nltk.translate.bleu_score.modified_precision(references, candidate1, n=2))) # modified 2-gram precision = 1/13\n",
    "float(nltk.translate.bleu_score.modified_precision(references, candidate1, n=1)) # modified 1-gram precision = 8/14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c765c2d",
   "metadata": {},
   "source": [
    "Note: Since we calculate modified $n-gram$ precision for a \"sentence\"(for  $\\LaTeX$ code, it might refer to a single sequence of tokens representing the candidate $\\LaTeX$ code for an equation, an image, a table, etc), in order to calculate $n-gram$ precision (denoted $p_n$ )for the whole document(collection of \"sentences\") we use the following formula:\n",
    "$$ p_n = \\frac{\\sum\\limits_{\\mathcal{C}\\in \\{ Candidates \\}}\\sum\\limits_{n-gram \\in \\mathcal{C}}count\\_clip(\\mathcal{n-gram})}{\\sum\\limits_{\\mathcal{C'}\\in \\{ Candidates \\}}\\sum\\limits_{n-gram \\in \\mathcal{C'}}count(\\mathcal{n-gram})} $$.\n",
    "\n",
    "<table><tr><th>Why should we consider a weighted average of $n-gram$ precisions over multiple $n$?</th></tr></table>\n",
    "It is obvious as $n$-increases(upto a certain point) modified $n-gram$ precision of a corpus(whole document) better relates to translation quality:\n",
    "\n",
    "+ *low $n$*: Checks the vocabulary of tokens/words used in the candidate, i.e. ensures the candidate doesn't use too many unrelated tokens, but it doesn't enforce token order and one might have a complete garbage candidate with correct tokens all jumbled up.\n",
    "\n",
    "+ *high $n$*: Checks and ensures proper token order and longer syntactic rules, but may be too harsh on minor deviations from \"desired\" references.\n",
    "\n",
    "Hence, we need to average out the modified precision scores for all $n\\leq N$(threshold).\n",
    "\n",
    "But...\n",
    "<figure>\n",
    "    <img src=\"image1.jpg\" alt=\"Alt text\" />\n",
    "    <figcaption><center><mark>Exponentially decaying $n-gram$ scores. Here $H_{i}:Human_{i} $,$S_{j}:System/Machine_{j}$</mark></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "as the image shows, modified $n$-gram scores decays rough exponentially with $n$ for human translators(\"good\") as well as machine translators(\"bad\").\n",
    "\n",
    "Therefore we use a heuristic wherein we take <mark>weighted average of logarithms of $p_n$</mark> for $n\\leq N$. (The baseline BLEU uses uniform weights)\n",
    "\n",
    "<mark>Sentence length</mark>: A candidate should neither be too short, nor too long, already the modified $n-gram$ precisions punish longer candidates that repeat a particular instance of a token too much, or use spurious words, **but**, too short candidates using the right words and orders(ex: a fragment of a reference) are spared, as seen in the following example.  Therefore we deploy a *brevity penalty* on longer sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae84ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'.split()\n",
    "reference2 = 'It is the guiding principle which gurantees the military forces always being under the command of the Party'.split()\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'.split()\n",
    "\n",
    "candidate1 = 'of the'.split() # very short and undesirable candidate\n",
    "references = [reference1, reference2, reference3]\n",
    "for i in range(1,3):\n",
    "    print(float(nltk.translate.bleu_score.modified_precision(references, candidate1, n=i))) # score is still 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060ae16",
   "metadata": {},
   "source": [
    "<mark>Definition $3$(*Brevity penalty*)</mark>: Let $r_s(\\mathcal{C}) = |\\epsilon| s.t. ||\\epsilon|-|\\mathcal{C}||=\\min\\limits_{\\mathcal{R}\\in \\{References\\}}||\\mathcal{C}|-|\\mathcal{R}||$. Call $r_s$ the *best match length*.\n",
    "Let $r$(*effective refernce corpus length*) be the sum of *best match lengths*($r_s$) of each sentence in the corpus, then $$BP(\\text{Brevity Penalty}) =\\begin{cases}\n",
    "1 & if \\ c > r\\\\\n",
    "e^{1-\\frac{r}{c}} & if \\ c \\leq  r\n",
    "\\end{cases}$$, where $c=|\\mathcal{C}|$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f97cb4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009118819655545162"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'.split()\n",
    "reference2 = 'It is the guiding principle which gurantees the military forces always being under the command of the Party'.split()\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'.split()\n",
    "\n",
    "candidate1 = 'of the'.split() # very short and undesirable candidate\n",
    "references = [reference1, reference2, reference3]\n",
    "cand_len = len(candidate1)\n",
    "closest_ref_len =  nltk.translate.bleu_score.closest_ref_length(references, cand_len)\n",
    "nltk.translate.bleu_score.brevity_penalty(closest_ref_len, cand_len) # brevity penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98922736",
   "metadata": {},
   "source": [
    "Final BLEU Metric:\n",
    "\n",
    "$$BLEU = BP. \\exp{(\\sum\\limits_{n=1}^{N}w_n \\log{p_n})}$$\n",
    "\n",
    "In the baseline metric, $w_n=\\frac{1}{N}$ and $N=4$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43d16562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7447490192819548"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "reference  = '\\dfrac{1}{\\sqrt{n} \\Sigma_{i=1}^{n} |i \\rangle'\n",
    "ref        = [char for char in reference]\n",
    "\n",
    "candidate1 = '\\frac{1}{\\sqrt{n} \\Sigma\\limits_{i=1}^{n} |i>'\n",
    "cand       = [char for char in candidate1]\n",
    "list_of_references = [[ref]]\n",
    "hypotheses = [cand]\n",
    "\n",
    "nltk.translate.bleu_score.corpus_bleu(list_of_references, hypotheses) # BLEU score, by default uniform weight and N=4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571e2bb",
   "metadata": {},
   "source": [
    "### ROUGE metric\n",
    " ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It incorporates Recall and Precision(the formula \"follows\" from the basic one in [here](#two)) in its multiple variants.ROUGE is actually a family of four different measures: `ROUGE-N`, `ROUGE-L`, `ROUGE-W` and `ROUGE-S`. \n",
    " \n",
    "We describe three of them(`ROUGE-N`, `ROUGE-L` and `ROUGE-W`) assisted with some minimal python implementation.\n",
    "\n",
    "### ROUGE-$N$:\n",
    "\n",
    "For a single reference $R$ and a candidate $C$, ROUGE_$N (R,C)$= $\\dfrac{ \\sum\\limits_{ distinct \\ ng \\in R,ng:  N-gram}Count_{match}(ng)}{ \\sum\\limits_{ng \\in R,ng: N-gram}Count(ng)}$.\n",
    "\n",
    "For the definition of $N$-gram refer [here](#one).. In the above formula, in the numerator, the summation goes over all distinct $N$-grams $ng$ in it. $Count_{match}(ng)$ refers to the maximum count of $ng$ co-occuring in the Reference as well as in the Candidate.\n",
    "\n",
    "***What should we do for multiple references?***\n",
    "\n",
    "We simply take the maximum over the pairwise ROUGE_$N$ scores for each reference $r_i$ and candidate $C$, i.e.,\n",
    "ROUGE-$N_{multi}$ = $\\max\\limits_{r_i}$ ROUGE-$N$ ($r_i$,$C$).\n",
    "\n",
    "In the python implementation, however, we do a *Jackknifing procedure*: Given $M$ references, we calculate ROUGE-$N_{multi}$ scores for each subset of $M-1$ references($M$ such choices) and take the average of the $M$ scores obtained. This fixes the problem of inflated score, where the candidate correlates highly with only a few of the references, and very poorly with others. \n",
    "\n",
    "We use the wonderful `rouge_score` library https://github.com/pltrdy/rouge uploaded to PYPI for implementations. (install using `pip install rouge-score`). We could also load the `rouge` module from `evaluate` library, but I figured out it uses `rouge_score` library too!)\n",
    "\n",
    "**ALERT**: The default tokenizer used in `rouge_score` discards non-alphanumeric characters and replaces them by white-spaces, so I made my tokenizer(`mytokenizer`) which basically performs `.split()` to create list of tokens. You can use your own tokenizer with appropriate sophistication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1691885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mytokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec73ca63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Score(precision=0.47619047619047616, recall=0.967741935483871, fmeasure=0.6382978723404255)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = [\"f ( z ) = \\sum _ { n = - \\infty } ^ { \\infty } f _ { n } e ^ { 2 \\pi i n z }\"]\n",
    "candidate = [ \"f ( z ) = \\sum _ { n = - \\infty } ^ { \\in \\{ \\pi i n z } , f _ { z } e ^ { 2 0 d ^ { 3 } , 4 0 , 7 2 0 s } , , , \\in ^ { 4 } , 7 , 9 2 0 n }\"]\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True, tokenizer = mytokenizer.DefaultTokenizer())\n",
    "scores = scorer.score(reference[0], candidate[0])\n",
    "for key in scores: # fmeasure is the ROUGE-N score\n",
    "    print(f'{key}: {scores[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80976459",
   "metadata": {},
   "source": [
    "### ROUGE-L:\n",
    "Given two sequences $X=[x_1, x_2\\ldots x_m]$ and $Y = [y_1, y_2, \\ldots, y_n]$, $LCS(X,Y)$ is the length of the longest a subsequence common between $X$ and $Y$, where a subsequence of $X$ is obtained from $X$ by deleting some(possibly none) $x_i(s)$ but preserving the original order(same with $Y$). It is a known exercise in dynamic programming to calculate $LCS$ of two strings.\n",
    "\n",
    "### Sentence-level LCS:\n",
    "Assume that both the reference and the prediction/candidate are single sentences. (*Why this distinction between single and multiple-sentence text?* Well, sometimes each sentence is semantically complete and two consecutive sentences might not have any related semantics whatsoever). View a sentence as a sequence of *words*(in $\\LaTeX$ code *words* for instance can be very naively considered as smallespiece of code delimited by white-spaces), so that each $x_i$ and $y_j$ in our definition is now a *word*\n",
    "\n",
    "We now define the $LCS$ based precion and recall scores as $P_{lcs} = \\dfrac{LCS(X,Y)}{n}$ and $R_{lcs} = \\dfrac{LCS(X,Y)}{m}$, where the length of sequence $X$(the reference) is $m$ and that of $Y$(the candidate) is $n$. We defined the weighted F-score as: ROUGE-L($X$,$Y$) :=  $\\dfrac{(1+\\beta^2)R_{lcs}P_{lcs}}{\\beta^2P_{lcs}+R_{lcs}}$ ,where the normal F-score is obtained when $\\beta=1$(this one is implemented in python), select high $\\beta>1$ for more weightage to $R_{lcs}$ and low $\\beta<1$ for more weightage to $P_{lcs}$.\n",
    "\n",
    "ROUGE-L has two obvious benefits over ROUGE-$N$:\n",
    "+ It doesn't require consecutive matches but *in-sequence* matches( so, no n-gram analysis needed)\n",
    "+ it automatically includes longest in-sequence common n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23392861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rougeL: Score(precision=0.4126984126984127, recall=0.8387096774193549, fmeasure=0.553191489361702)\n"
     ]
    }
   ],
   "source": [
    "# ROUGE-L on the previous example\n",
    "reference = [\"f ( z ) = \\sum _ { n = - \\infty } ^ { \\infty } f _ { n } e ^ { 2 \\pi i n z }\"]\n",
    "candidate = [ \"f ( z ) = \\sum _ { n = - \\infty } ^ { \\in \\{ \\pi i n z } , f _ { z } e ^ { 2 0 d ^ { 3 } , 4 0 , 7 2 0 s } , , , \\in ^ { 4 } , 7 , 9 2 0 n }\"]\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True, tokenizer = mytokenizer.DefaultTokenizer())\n",
    "scores = scorer.score(reference[0], candidate[0])\n",
    "for key in scores: # fmeasure is the ROUGE-L score\n",
    "    print(f'{key}: {scores[key]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c616f8",
   "metadata": {},
   "source": [
    "See below one of the examples of a situation where ROUGE-L differentiates between two candidates of varying quality which ROUGE-$N$ (here $N=2$) cannot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d63ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For candidate1:\n",
      "rougeL: Score(precision=0.75, recall=0.75, fmeasure=0.75)\n",
      "For candidate2:\n",
      "rougeL: Score(precision=0.5, recall=0.5, fmeasure=0.5)\n",
      "\n",
      "For candidate1:\n",
      "rouge2: Score(precision=0.3333333333333333, recall=0.3333333333333333, fmeasure=0.3333333333333333)\n",
      "For candidate2:\n",
      "rouge2: Score(precision=0.3333333333333333, recall=0.3333333333333333, fmeasure=0.3333333333333333)\n"
     ]
    }
   ],
   "source": [
    "reference = \"police killed the gunman\"\n",
    "candidate1 = \"police kill the gunman\"\n",
    "candidate2 = \"the gunman kill police\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True, tokenizer = mytokenizer.DefaultTokenizer())\n",
    "scores = scorer.score(reference, candidate1)\n",
    "print(\"For candidate1:\")\n",
    "for key in scores: # fmeasure is the ROUGE-L score\n",
    "    print(f'{key}: {scores[key]}')\n",
    "scores = scorer.score(reference, candidate2)\n",
    "print(\"For candidate2:\")\n",
    "for key in scores: # fmeasure is the ROUGE-L score\n",
    "    print(f'{key}: {scores[key]}')\n",
    "\n",
    "print()\n",
    "\n",
    "# ROUGE-2 can't differentiate the two candidates\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True, tokenizer = mytokenizer.DefaultTokenizer())\n",
    "scores = scorer.score(reference, candidate1)\n",
    "print(\"For candidate1:\")\n",
    "for key in scores: # fmeasure is the ROUGE-L score\n",
    "    print(f'{key}: {scores[key]}')\n",
    "scores = scorer.score(reference, candidate2)\n",
    "print(\"For candidate2:\")\n",
    "for key in scores: # fmeasure is the ROUGE-L score\n",
    "    print(f'{key}: {scores[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d5cf3",
   "metadata": {},
   "source": [
    "### Summary-Level LCS:\n",
    "\n",
    "Now, instead of being single sentences, lets say both the candidate the reference are composed of multiple sentences.\n",
    "Say, we have reference $R=[r_1,r_2,\\ldots , r_u]$ of $u$ sentences containing $m$ total words and a candidate $C$ of $v$ sentences containing $n$ total words. Then we calculate the $LCS$ based precision and recall in this case in the following way:\n",
    "\n",
    "$R_{lcs}=\\dfrac{\\sum\\limits_{i=1}^u LCS_{\\cup}(r_i,C)}{m}$ and $P_{lcs}=\\dfrac{\\sum\\limits_{i=1}^u LCS_{\\cup}(r_i,C)}{n}$, where\n",
    "$LCS_{\\cup}(r_i,C)$ is the length of the union of $LCSs$ between each $c_j \\in C$ and $r_i$.\n",
    "Let me illustrate this by an example: say $C=[c_1,c_2]$, where $c_1 = w_1 w_3 w_8 w_9 w_5$ and $c_2=w_1w_2w_6w_7w_8$ (each $w_k$ is a word), and let $r_i=w_1w_2w_3w_4w_5$. We have $lcs$ of $r_i$ and $c_1$ as $w_1w_3w_5$ and $lcs$ of $r_i$ and $c_2$ as $w_1w_2$, therefore the *union lcs* of $r_i$ and $C$ is $w_1w_2w_3w_5$ and $LCS_{\\cup}(r_i,C)=4$.\n",
    "\n",
    "Now the summary-level ROUGE-L score is defined as the weighted *F-measure* of $P_{lcs}$ and $R_{lcs}$, i.e. $ROUGE-L(C,R):=F_{lcs} = \\dfrac{(1+\\beta^2)R_{lcs}P_{lcs}}{\\beta^2P_{lcs}+R_{lcs}}$ . Significance of $\\beta$ is same as in ROUGE-$N$.\n",
    "\n",
    "For multiple references, we follow a jackknifing procedure as earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a7107b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge2: Score(precision=0.5, recall=0.8269230769230769, fmeasure=0.6231884057971014)\n"
     ]
    }
   ],
   "source": [
    "reference = \"f ( z ) = \\sum _ { n = - \\infty } ^ { \\infty } f _ { n } e ^ { 2 \\pi i n z }. \\varepsilon ^ { \\mu \\nu \\alpha \\beta } \\partial _ { \\nu } B _ { \\alpha \\beta } = 0 .\"\n",
    "candidate =  \"f ( z ) = \\sum _ { n = - \\infty } ^ { \\in \\{ \\pi i n z } , f _ { z } e ^ { 2 0 d ^ { 3 } , 4 0 , 7 2 0 s } , , , \\in ^ { 4 } , 7 , 9 2 0 n }.\\varepsilon ^ { 2 3 s \\n u \\alpha \\beta } \\partial _ { \\nu } B _ { \\alpha \\beta } = 0 .\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True,split_summaries=True, tokenizer = mytokenizer.DefaultTokenizer())\n",
    "scores = scorer.score(reference, candidate)\n",
    "for key in scores: # fmeasure is the ROUGE-L score\n",
    "    print(f'{key}: {scores[key]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec37aa",
   "metadata": {},
   "source": [
    "### ROUGE-W:\n",
    "\n",
    "The $LCS$ base *F-measure* is quite good, but there is also an inherent difficulty that it doesn't distinguish between *spatially* different *LCSs*. For e.g. let $X = [A,B,C,D,E,F,G]$ be a reference and $Y_1=[A,B,C,D,H,I,K]$ and $Y_2=[A,H,B,K,C,I,D]$ be two candidates. We have $LCS(X,Y_1)=LCS(X,Y_2)=4$ and $n=m=7$, hence both have same *ROUGE-L* scores, but its obvious that $Y_1$ is more favorable. To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS, update weight according to a new (consecutive)match and give more weight to larger consecutive matches. Finally the WLCS is the sum of weights given to each patch of consecutive matches between the reference and candidate sentence.\n",
    "\n",
    "We use the following dynamic programming algorithm to calculate *WLCS(Weighted LCS)* between two sentences $X$ and $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d327911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wlcs(x, y, weight=None):\n",
    "    m=len(x)\n",
    "    n=len(y)\n",
    "    c=[[0 for x in range(n)] for x in range(m)]\n",
    "    w = [[0 for x in range(n)] for x in range(m)]\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,n):\n",
    "            # initialize the tables\n",
    "            c[i][j]=0 # c is the DP table that computes the WLCS score\n",
    "            w[i][j]=0\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,n):\n",
    "            if (x[i]==y[j]):\n",
    "                k = w[i-1][j-1] # length of current number of consecutive matches ending at position i-1 and j-1\n",
    "                update = func(k+1,weight)-func(k,weight) # update the current weight to the consecutive matches by f(k+1)-f(k), where f is an easily invertible function we choose \n",
    "                c[i][j] = c[i-1][j-1] + update\n",
    "                w[i][j] = k + 1 # remember the length of consecutive matches at position i and j\n",
    "            else:\n",
    "                c[i][j] = max(c[i-1][j],c[i][j-1])\n",
    "                w[i][j] = 0 # no match at position i and j, so length of current number of consecutive matches ending at these position is 0!\n",
    "    return c[m-1][n-1] # the WLCS of x and y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee17230",
   "metadata": {},
   "source": [
    "Note that the weighting function *$f$(or func)* must have the property that $f(x+y) > f(x) + f(y)$ for any positive integers $x$\n",
    "and $y$. In other words, consecutive matches are awarded more scores than non-consecutive matches. We also require $f$ to be easily invertible (we generally choose functions of the form $f(x)=x^{weight}, weight>0$. Then we define the *WLCS* based precision and recall as (Let $X$ be the reference and $Y$ be the candidate):\n",
    "\n",
    "$P_{lcs}= f^{-1}( \\dfrac{WLCS(X,Y)}{f(n)})$ and $R_{lcs}= f^{-1}( \\dfrac{WLCS(X,Y)}{f(m)})$\n",
    "\n",
    "Finally, *ROUGE-W* score is defined as the weighted *F-measure* of $P_{lcs}$ and $R_{lcs}$, i.e. $ROUGE-W(X,Y):=F_{lcs} = \\dfrac{(1+\\beta^2)R_{lcs}P_{lcs}}{\\beta^2P_{lcs}+R_{lcs}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad9ebc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WLCS of candidate 1 w.r.t. reference is:  16.0\n",
      "ROUGE-W score of candidate 1 is:  0.5714285714285714\n",
      "\n",
      "WLCS of candidate 1 w.r.t. reference is:  4.0\n",
      "ROUGE-W score of candidate 1 is:  0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def func(x,weight,inverse=False):\n",
    "    if inverse:\n",
    "        return math.pow(x,1/weight)\n",
    "    else:\n",
    "        return math.pow(x,weight)\n",
    "    \n",
    "def rougew(x,y,weight=None):\n",
    "    m=len(x)\n",
    "    n=len(y)\n",
    "    wlcs1 = wlcs(x,y,weight=weight)\n",
    "    P = func(wlcs1/func(n,weight),weight=weight, inverse=True)\n",
    "    R = func(wlcs1/func(m,weight),weight=weight, inverse=True)\n",
    "    return (2*P*R/(P+R))\n",
    "\n",
    "\n",
    "reference=\"A B C D E F G\".split()\n",
    "candidate1 = \"A B C D H I K\".split()\n",
    "candidate2 = \"A H B K C I D\".split()\n",
    "\n",
    "# ROUGE-W clearly distinguishes the two candidates!\n",
    "print(\"WLCS of candidate 1 w.r.t. reference is: \",wlcs(reference,candidate1,weight=2))\n",
    "print(\"ROUGE-W score of candidate 1 is: \",rougew(reference,candidate1,weight=2))\n",
    "print()\n",
    "print(\"WLCS of candidate 1 w.r.t. reference is: \",wlcs(reference,candidate2,weight=2))\n",
    "print(\"ROUGE-W score of candidate 1 is: \",rougew(reference,candidate2,weight=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b736126",
   "metadata": {},
   "source": [
    "## Classification related\n",
    "### Accuracy\n",
    "\n",
    "<mark> Definition (*Confusion Matrix*)</mark>:The columns of the matrix represent the instances of the predicted classes predicted by a classifier and the rows represent the instances of the actual class. It is used to visualize the performance of a classifier. Note that the role of rows and columns can be inverted in some representations.\n",
    "\n",
    "Let us work with a binary classifier for now, with two classes namely **Positive** and **Negative**.\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & \\textbf{Predicted Negative} & \\textbf{Predicted Positive} \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Negative} & TN & FP \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Positive} & FN & TP \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "where $TN: True \\ Negative$, $TP: True \\ Positive$, $FN:False \\ Negative$, $FP: False \\ Positive$.\n",
    "\n",
    "Then *Accuracy* is then defined as : $ = {{TP + TN} \\over {TP + TN + FP + FN}}$\n",
    "The following is a logistic regression model which uses artificial train and test set. <mark>You can ignore all the code and simply look at the confusion matrix to calculate accuracy.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd1e5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "              Predicted Negative   Predicted Positive\n",
      "Actual Negative      15                  1\n",
      "Actual Positive      0                  14\n",
      "\n",
      "Accuracy of the classifier:\n",
      "Accuracy = 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, \n",
    "                            n_redundant=0, random_state=42, n_classes=2)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()  # Extract values from the confusion matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"              Predicted Negative   Predicted Positive\")\n",
    "print(f\"Actual Negative      {tn}                  {fp}\")\n",
    "print(f\"Actual Positive      {fn}                  {tp}\")\n",
    "\n",
    "print(\"\\nAccuracy of the classifier:\")\n",
    "print(f\"Accuracy = {accuracy:.2%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18ce45",
   "metadata": {},
   "source": [
    "**From now on, we will cook up arbitary confusion matrices, just to illustrate the metric, without actually spawning the classifier!**\n",
    "\n",
    "Note: Accuracy alone is not a sufficient metric of performance as shown by the following example:\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & \\textbf{Predicted Negative} & \\textbf{Predicted Positive} \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Negative} & 0 & 5 \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Positive} & 0 & 95 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "The classfier can have $0.95$ accuracy predicting \"*Positive*\" always!\n",
    "\n",
    "### Precision\n",
    "<mark>Definition</mark>: Fraction of True positives to all predicted positives.\n",
    "$$ precision={{TP} \\over {TP + FP}}$$.\n",
    "\n",
    "This roughly translates to the classifier's \"confidence\" in predicting positives--important where False positives are very costly.\n",
    "\n",
    "<a id='two'></a>\n",
    "### Recall\n",
    "<mark> Definition</mark>: Fraction of True positives to actual positives.\n",
    "$$Recall = {{TP} \\over {TP + FN}}$$\n",
    "\n",
    "Recall is crucial in applications where missing positive cases (False Negatives) has serious consequences: Medical diagnosis, Disaster alerts, etc.\n",
    "\n",
    "### F1-Score\n",
    "<mark>Definition</mark>: Harmonic mean of precision and recall.\n",
    "$$F1-score = 2*\\frac{Precision*Recall}{Precision+Recall}$$\n",
    "\n",
    "Precision focuses on the quality of positive predictions (How many of the predicted positives are correct?), whereas Recall focuses on the quantity of true positives identified (How many of the actual positives were found?).\n",
    "Together, they provide a fuller picture of a classifier's performance and are often combined using the F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d9f87",
   "metadata": {},
   "source": [
    "Main Credits @ https://python-course.eu/machine-learning/evaluation-metrics.php and @ https://dl.acm.org/doi/10.3115/1073083.1073135 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
